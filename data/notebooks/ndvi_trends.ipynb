{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDVI Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'quote' from 'dask.base' (c:\\Users\\kathr\\clean-and-green-philly\\.venv\\Lib\\site-packages\\dask\\base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstac\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplanetary_computer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kathr\\clean-and-green-philly\\.venv\\Lib\\site-packages\\odc\\stac\\__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"STAC Item -> ODC Dataset[eo3].\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# isort:skip  this has to be 1st import\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_rio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m configure_rio, configure_s3_access\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RasterBandMetadata, RasterLoadParams, RasterSource\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mdtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     ConversionConfig,\n\u001b[32m      9\u001b[39m     ParsedItem,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     parse_items,\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kathr\\clean-and-green-philly\\.venv\\Lib\\site-packages\\odc\\loader\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTools for constructing xarray objects from parsed metadata.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chunked_load, resolve_chunk_shape\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_driver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reader_driver, register_driver, unregister_driver\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     resolve_dst_dtype,\n\u001b[32m      9\u001b[39m     resolve_dst_nodata,\n\u001b[32m     10\u001b[39m     resolve_load_cfg,\n\u001b[32m     11\u001b[39m     resolve_src_nodata,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kathr\\clean-and-green-philly\\.venv\\Lib\\site-packages\\odc\\loader\\_builder.py:30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_dask_collection\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_chunks\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quote, tokenize\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhighlevelgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HighLevelGraph\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Key\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'quote' from 'dask.base' (c:\\Users\\kathr\\clean-and-green-philly\\.venv\\Lib\\site-packages\\dask\\base.py)"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import odc.stac\n",
    "import pandas as pd\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import panel as pn\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from rasterio.mask import mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Nissim has pointed out potentiality for time series, which I decided to investigate here.\n",
    "I wrote the code under his guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(catalog, bbox:list[float], datetime:str, cloudy_less_than:float):\n",
    "\t# Load all selected items (tiles) into a list of datasets\n",
    "\tbands_of_interest = [\"red\", \"green\", \"blue\", \"nir\", \"swir16\"]\n",
    "\tdatasets = []\n",
    "\t\n",
    "\tsearch = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": cloudy_less_than}}\n",
    "\t)\n",
    "\n",
    "\titems = search.items()\n",
    "\titems = list(items)\n",
    "\n",
    "\tfor item in items:\n",
    "\n",
    "\t\tds_tile = odc.stac.stac_load(\n",
    "\t\t\titems=[item],\n",
    "\t\t\tbands=bands_of_interest,\n",
    "\t\t\tbbox=bbox,\n",
    "\t\t\tresolution=5,\n",
    "\t\t\tchunks={},  # Enable Dask for memory efficiency\n",
    "\t\t)\n",
    "\t\tdatasets.append(ds_tile)\n",
    "\n",
    "\t\t# Load all selected items (tiles) into a list of datasets\n",
    "\tbands_of_interest = [\"red\", \"green\", \"blue\", \"nir\", \"swir16\"]\n",
    "\tdatasets = []\n",
    "\n",
    "\tfor item in items:\n",
    "\t\tds_tile = odc.stac.stac_load(\n",
    "\t\t\titems=[item],\n",
    "\t\t\tbands=bands_of_interest,\n",
    "\t\t\tbbox=bbox,\n",
    "\t\t\tresolution=5,\n",
    "\t\t\tchunks={},  # Enable Dask for memory efficiency\n",
    "\t\t)\n",
    "\t\tdatasets.append(ds_tile)\n",
    "\tprint(\"=== completed dataset collection ===\")\n",
    "\treturn xr.concat(datasets, dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NDVI(nir, red):\n",
    "    \"\"\"\n",
    "    Calculate the NDVI from the NIR and red landsat bands\n",
    "    \"\"\"\n",
    "\n",
    "    if red.max() > 1:\n",
    "        red = red / 10000.0\n",
    "    if nir.max() > 1:\n",
    "        nir = nir / 10000.0\n",
    "\n",
    "    # Calculate NDVI\n",
    "    ndvi = (nir - red) / (nir + red)\n",
    "\n",
    "    # Mask invalid values (divide by zero or NaN)\n",
    "    # ndvi = np.nan_to_num(ndvi, nan=-9999)  # Replace NaN with a placeholder\n",
    "    \n",
    "    # Return\n",
    "    return np.nanmedian(ndvi)\n",
    "\n",
    "# NDVI_city = calculate_NDVI(nir, red)\n",
    "# median_NDVI_city = np.nanmedian(NDVI_city)\n",
    "# median_NDVI_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I investigated using the city's median NDVI index using time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_limits = gpd.read_file(\"./City_Limits.geojson\")\n",
    "city_limits = city_limits.to_crs(32618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndvi_trends(datasets, clip=city_limits):\n",
    "\ttimestamps = datasets.time.values\n",
    "\n",
    "\n",
    "\tres = []\n",
    "\tfor timestamp in timestamps:\n",
    "\t\tprint(f\"=== Median NDVI calculated for {timestamp} ===\")\n",
    "\t\tquery = datasets.sel(time=timestamp).to_array(dim=\"band\").compute()\n",
    "\t\tquery = query.rio.clip([clip], query.rio.crs, drop=True)\n",
    "\t\tred = query.red.values\n",
    "\t\tnir = query.nir.values\t\n",
    "\t\t\n",
    "\t\tres.append(calculate_NDVI(nir, red))\n",
    "\t\t\n",
    "\t\tprint(f\"=== collection complete for {timestamp} ===\")\n",
    "\t\n",
    "\treturn pd.DataFrame({'timestamp': timestamps, 'median ndvi': res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,  # Automatically signs requests\n",
    ")\n",
    "# List available collections\n",
    "all_collections = [i.id for i in catalog.get_collections()]\n",
    "sentinel_collections = [collection for collection in all_collections if \"sentinel\" in collection]\n",
    "print(\"Available Sentinel Collections:\", sentinel_collections)\n",
    "\n",
    "# Corrected Query for Sentinel-2 with Cloud Cover Filter\n",
    "bbox = [-75.2803, 39.8670, -74.9557, 40.1379]  # Philadelphia bounding box\n",
    "datetime = \"2022-08-01/2024-07-31\"  # Summer 2024\n",
    "cloudy_less_than = 10  # Percent cloud cover threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took a while, but I did found evidence to suggest that there are fluctuations here after computing the ndvi for each point for the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets(catalog=catalog, bbox=bbox, datetime=datetime, cloudy_less_than=cloudy_less_than)\n",
    "trends = ndvi_trends(datasets=datasets)\n",
    "trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends['timestamp'] = datasets.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends.to_parquet('ndvi_trends.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends.drop(columns=['time'], inplace=True)\n",
    "# trends.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot seems to indicate this to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends.plot.scatter('timestamp', 'ndvi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I fit a quick sinodal curve and plotted it as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# 1. Define your sinusoidal model\n",
    "def seasonal_sin(t, offset, amplitude, phase):\n",
    "    \"\"\"\n",
    "    Simple seasonal sine function:\n",
    "      NDVI(t) = offset + amplitude * sin((2*pi/period)*t + phase)\n",
    "    \"\"\"\n",
    "    period = 365.0  # days, if you expect a 1-year cycle\n",
    "    return offset + amplitude * np.sin((2 * np.pi / period) * t + phase)\n",
    "\n",
    "# 2. Convert your timestamps to a numeric scale\n",
    "#    Suppose 'trends' is a pandas DataFrame with columns 'timestamp' (datetime) and 'ndvi'\n",
    "trends['t_ordinal'] = trends['timestamp'].map(pd.Timestamp.toordinal)\n",
    "x = trends['t_ordinal'].values.astype(float)\n",
    "y = trends['ndvi'].values.astype(float)\n",
    "\n",
    "# 3. Provide an initial guess for offset, amplitude, and phase\n",
    "initial_guess = [0.2, 0.1, 0.0]  # for example\n",
    "\n",
    "# 4. Fit the model\n",
    "popt, pcov = curve_fit(seasonal_sin, x, y, p0=initial_guess)\n",
    "offset, amplitude, phase = popt\n",
    "print(\"Fitted parameters:\")\n",
    "print(\"offset  =\", offset)\n",
    "print(\"amplitude =\", amplitude)\n",
    "print(\"phase    =\", phase)\n",
    "\n",
    "# 5. Generate fitted values\n",
    "trends['ndvi_sine_fit'] = seasonal_sin(x, offset, amplitude, phase)\n",
    "\n",
    "# 6. Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(trends['timestamp'], trends['ndvi'], label='Data', s=20)\n",
    "plt.plot(trends['timestamp'], trends['ndvi_sine_fit'], color='red', label='Sine Fit')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NDVI\")\n",
    "plt.title(\"Seasonal NDVI Fit (Sine Wave)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Definitely, a more sinodal pattern here. Potentially, a periodic time series?\n",
    "- Computation of median ndvi is a drag on the system but not to much at the moment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
